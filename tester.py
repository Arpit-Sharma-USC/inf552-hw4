import pandas as pd
from sklearn.utils import shuffle
import numpy as np
import matplotlib.pyplot as plt

active_errors=[0.13771186, 0.125,      0.04661017, 0.02966102, 0.0190678,  0.01271186,
 0.01271186, 0.01059322 ,0.01059322 ,0.01059322, 0.01059322 ,0.01059322,
 0.01059322, 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322, 0.01059322, 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322, 0.01059322, 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322, 0.01059322, 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322, 0.01059322, 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322, 0.01059322 ,0.01059322 ,0.01059322 ,0.01059322 ,0.01059322,
 0.01059322, 0.01059322 ,0.01059322 ,0.01059322 ,0.01271186, 0.01271186,
 0.01059322, 0.01059322 ,0.01059322, 0.01271186 ,0.01271186,0.01271186]

passive_errors=[0.125  ,    0.05720339, 0.03813559, 0.02330508 ,0.0190678,  0.02330508,
 0.01694915 ,0.01483051, 0.01483051 ,0.01483051 ,0.01694915, 0.01694915,
 0.01694915 ,0.01694915 ,0.01694915 ,0.02118644 ,0.01694915 ,0.01694915,
 0.01694915 ,0.01483051 ,0.02330508 ,0.01483051 ,0.02330508 ,0.02754237,
 0.02754237 ,0.02118644 ,0.02118644 ,0.02118644 ,0.02118644 ,0.02330508,
 0.02542373 ,0.02542373 ,0.01483051 ,0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186, 0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186 ,0.01271186, 0.01271186 ,0.01271186 ,0.01271186,
 0.01271186 ,0.01271186, 0.01271186, 0.01271186 ,0.01271186, 0.01271186]

print(active_errors)
print(len(active_errors))

x_axis_list=[]

for i in range(1,91):
    x_axis_list.append(i)

x_axis = np.array(x_axis_list)

plt.title('Average Test-errors for 90 SVMs- Passive vs Active Learning')
plt.xlabel('Number of SVMs')
plt.ylabel('Test Error')
plt.plot(x_axis, active_errors,'r',label="active learning")
plt.plot(x_axis, passive_errors,'b',label="passive learning")
plt.legend()
plt.show()


#passive
# [0.125      0.05720339 0.03813559 0.02330508 0.0190678  0.02330508
#  0.01694915 0.01483051 0.01483051 0.01483051 0.01694915 0.01694915
#  0.01694915 0.01694915 0.01694915 0.02118644 0.01694915 0.01694915
#  0.01694915 0.02330508 0.02330508 0.01483051 0.02330508 0.02754237
#  0.02754237 0.02118644 0.02118644 0.02118644 0.02118644 0.02330508
#  0.02542373 0.02542373 0.01483051 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186
#  0.01271186 0.01271186 0.01271186 0.01271186 0.01271186 0.01271186]


#active
#  [0.13771186 0.125      0.04661017 0.02966102 0.0190678  0.01271186
#  0.01271186 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322 0.01059322
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01271186 0.01271186
#  0.01059322 0.01059322 0.01059322 0.01059322 0.01059322]
